{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**PROBLEM DESCRIPTION**\n\nThis notebook demostrates a **sequence classification of IMDB movie reviews** dataset by creating a simple LSTM based classifier. \n\nEach movie review is a variable sequence of words, and the tone of each movie review must be classified. The large movie reviews dataset (sometimes referred to as the IMDB dataset) contains *25,000 film reviews (good or bad) for training and 25,000 reviews for testing*. The problem is deciding whether a given movie review is positive or negative. The data were collected by researchers at Stanford  and  used in a 2011 paper that used 50-50  data  for training and testing. An accuracy of 88.89% is achieved. \n\nHere, a built-in dataset of IMDB movie reviews is used. Keras provide several built-in dataset, one of them is - **imdb.load_data()**. ","metadata":{}},{"cell_type":"markdown","source":"**Import modules**\n\nLet's start off with the basic step of importing all the relevant modules and functions required for this particular classifier.","metadata":{}},{"cell_type":"code","source":"import numpy\nfrom keras.datasets import imdb #built-in dataset\nfrom keras.models import Sequential \nimport pandas as pd\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding #to define the word embedding\nfrom keras.preprocessing import sequence","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:12:00.978031Z","iopub.execute_input":"2022-01-02T16:12:00.978845Z","iopub.status.idle":"2022-01-02T16:12:01.002942Z","shell.execute_reply.started":"2022-01-02T16:12:00.978804Z","shell.execute_reply":"2022-01-02T16:12:01.001832Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Next, take the amount of data you want to use for this particular model. The original dataset contains 50,000 movie reviews. But, it can be restricted as per the requirement. Here, we can select the top 10,000 words of the plathora of this dataset. \n\nThen split the dataset into train (50%) and test (50%) sets.","metadata":{}},{"cell_type":"code","source":"top_words = 10000\n(X_train, y_train), (X_test, y_test) = df(num_words=top_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:06:57.746979Z","iopub.execute_input":"2022-01-02T16:06:57.747243Z","iopub.status.idle":"2022-01-02T16:07:00.421585Z","shell.execute_reply.started":"2022-01-02T16:06:57.747216Z","shell.execute_reply":"2022-01-02T16:07:00.420602Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Now, we need to pad sequences so that they can be of same length for modelling. For sure, when there is no information after this process, the model will take zero value for them. ","metadata":{}},{"cell_type":"code","source":"# truncate and pad input sequences\nmax_review_length = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:57:05.000666Z","iopub.status.idle":"2022-01-02T15:57:05.001410Z","shell.execute_reply.started":"2022-01-02T15:57:05.001118Z","shell.execute_reply":"2022-01-02T15:57:05.001146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define the LSTM model**\n\nThe first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Subsequently, you can add more than one LSTM layer. Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n\nBecause it is a *binary classification problem*, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The number of epochs and batch size can be increased as per the requirement. Here, we have taken epoch of 10 and batch size of 64.","metadata":{}},{"cell_type":"code","source":"# create the model\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:01:46.277782Z","iopub.execute_input":"2022-01-02T16:01:46.278058Z","iopub.status.idle":"2022-01-02T16:01:46.305746Z","shell.execute_reply.started":"2022-01-02T16:01:46.278032Z","shell.execute_reply":"2022-01-02T16:01:46.304599Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Once the model is created, then we can test the performance on unseen reviews. ","metadata":{}},{"cell_type":"code","source":"# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:57:05.005166Z","iopub.status.idle":"2022-01-02T15:57:05.005517Z","shell.execute_reply.started":"2022-01-02T15:57:05.005329Z","shell.execute_reply":"2022-01-02T15:57:05.005345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The complete code can be seen here. Let's see how the model turns out to be. ","metadata":{}},{"cell_type":"code","source":"# LSTM for sequence classification in the IMDB dataset\nimport numpy\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\n# load the dataset but only keep the top n words, zero the rest\ntop_words = 5000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n\n# truncate and pad input sequences\nmax_review_length = 10000\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n\n# create the model\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(X_train, y_train, epochs=10, batch_size=64)\n\n# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:13:17.250445Z","iopub.execute_input":"2022-01-02T16:13:17.251352Z","iopub.status.idle":"2022-01-02T16:13:37.328715Z","shell.execute_reply.started":"2022-01-02T16:13:17.251286Z","shell.execute_reply":"2022-01-02T16:13:37.327243Z"},"trusted":true},"execution_count":20,"outputs":[]}]}