{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**PROBLEM DESCRIPTION**\n\nThis notebook demostrates a **sequence classification of IMDB movie reviews** dataset by creating a simple LSTM based classifier. \n\nEach movie review is a variable sequence of words, and the tone of each movie review must be classified. The large movie reviews dataset (sometimes referred to as the IMDB dataset) contains *25,000 film reviews (good or bad) for training and 25,000 reviews for testing*. The problem is deciding whether a given movie review is positive or negative. The data were collected by researchers at Stanford  and  used in a 2011 paper that used 50-50  data  for training and testing. An accuracy of 88.89% is achieved. \n","metadata":{}},{"cell_type":"markdown","source":"**Import modules**\n\nLet's start off with the basic step of importing all the relevant modules and functions required for this particular classifier.","metadata":{}},{"cell_type":"code","source":"import pandas as pd    # to load dataset\nimport numpy as np     # for mathematic equation\nfrom nltk.corpus import stopwords   # to get collection of stopwords\nfrom sklearn.model_selection import train_test_split       # for splitting dataset\nfrom tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\nfrom tensorflow.keras.models import Sequential     # the model\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\nfrom tensorflow.keras.callbacks import ModelCheckpoint   # save model\nfrom tensorflow.keras.models import load_model   # load saved model\n","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:50:09.363219Z","iopub.execute_input":"2022-01-02T17:50:09.363777Z","iopub.status.idle":"2022-01-02T17:50:09.371669Z","shell.execute_reply.started":"2022-01-02T17:50:09.363733Z","shell.execute_reply":"2022-01-02T17:50:09.370765Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"Firstly, let's learn about our dataset. For this we need to import the data and convert it into a Pandas' dataframe.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:45:37.419214Z","iopub.execute_input":"2022-01-02T17:45:37.420176Z","iopub.status.idle":"2022-01-02T17:45:38.152295Z","shell.execute_reply.started":"2022-01-02T17:45:37.420110Z","shell.execute_reply":"2022-01-02T17:45:38.151494Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"**Load and Clean Dataset**\n\nIn the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in load_dataset() function, beside loading the dataset using pandas, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.","metadata":{}},{"cell_type":"code","source":"english_stops = set(stopwords.words('english')) #declaring stop words\n\ndef load_dataset():\n    df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n    x_data = df['review']       # Reviews/Input\n    y_data = df['sentiment']    # Sentiment/Output\n\n    # PRE-PROCESS REVIEW\n    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n    \n    # ENCODE SENTIMENT -> 0 & 1\n    y_data = y_data.replace('positive', 1)\n    y_data = y_data.replace('negative', 0)\n\n    return x_data, y_data\n\nx_data, y_data = load_dataset()\n\nprint('Reviews')\nprint(x_data, '\\n')\nprint('Sentiment')\nprint(y_data)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:46:04.003216Z","iopub.execute_input":"2022-01-02T17:46:04.003927Z","iopub.status.idle":"2022-01-02T17:46:17.218754Z","shell.execute_reply.started":"2022-01-02T17:46:04.003877Z","shell.execute_reply":"2022-01-02T17:46:17.217776Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"The total review count in above data is - ","metadata":{}},{"cell_type":"code","source":"df.sentiment.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:46:17.220949Z","iopub.execute_input":"2022-01-02T17:46:17.221195Z","iopub.status.idle":"2022-01-02T17:46:17.236481Z","shell.execute_reply.started":"2022-01-02T17:46:17.221162Z","shell.execute_reply":"2022-01-02T17:46:17.235411Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"The data can be visualized by using features of matplotlib library. By doing so, we can the following results,","metadata":{}},{"cell_type":"code","source":"Sentiment_count=df.groupby('sentiment').count()\nplt.bar(Sentiment_count.index.values, Sentiment_count['review'])\nplt.xlabel('Review Sentiments')\nplt.ylabel('Number of Review')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:46:25.260663Z","iopub.execute_input":"2022-01-02T17:46:25.261516Z","iopub.status.idle":"2022-01-02T17:46:25.470051Z","shell.execute_reply.started":"2022-01-02T17:46:25.261473Z","shell.execute_reply":"2022-01-02T17:46:25.469069Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"Next, take the amount of data you want to use for this particular model. The original dataset contains 50,000 movie reviews. But, it can be restricted as per the requirement. \n\nThen split the dataset into train (70%) and test (30%) sets.","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3)\n\nprint('Train Set')\nprint(x_train, '\\n')\nprint(x_test, '\\n')\nprint('Test Set')\nprint(y_train, '\\n')\nprint(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:48:18.254604Z","iopub.execute_input":"2022-01-02T17:48:18.255298Z","iopub.status.idle":"2022-01-02T17:48:18.359485Z","shell.execute_reply.started":"2022-01-02T17:48:18.255246Z","shell.execute_reply":"2022-01-02T17:48:18.358561Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def get_max_length():\n    review_length = []\n    for review in x_train:\n        review_length.append(len(review))\n\n    return int(np.ceil(np.mean(review_length)))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:48:30.245272Z","iopub.execute_input":"2022-01-02T17:48:30.245626Z","iopub.status.idle":"2022-01-02T17:48:30.250302Z","shell.execute_reply.started":"2022-01-02T17:48:30.245589Z","shell.execute_reply":"2022-01-02T17:48:30.249669Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"\n**Tokenize and Pad/Truncate Reviews**\n\nA Neural Network only accepts numeric data, so we need to encode the reviews.\n\nEach reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.\n","metadata":{}},{"cell_type":"code","source":"# ENCODE REVIEW\ntoken = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\ntoken.fit_on_texts(x_train)\nx_train = token.texts_to_sequences(x_train)\nx_test = token.texts_to_sequences(x_test)\n\nmax_length = get_max_length()\n\nx_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\nx_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n\ntotal_words = len(token.word_index) + 1   # add 1 because of 0 padding\n\nprint('Encoded X Train\\n', x_train, '\\n')\nprint('Encoded X Test\\n', x_test, '\\n')\nprint('Maximum review length: ', max_length)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:48:46.332825Z","iopub.execute_input":"2022-01-02T17:48:46.333158Z","iopub.status.idle":"2022-01-02T17:48:55.182128Z","shell.execute_reply.started":"2022-01-02T17:48:46.333123Z","shell.execute_reply":"2022-01-02T17:48:55.181276Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"**Define the LSTM model**\n\nThe first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Subsequently, you can add more than one LSTM layer. Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n\nBecause it is a *binary classification problem*, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The number of epochs and batch size can be increased as per the requirement. ","metadata":{}},{"cell_type":"code","source":"# LSTM Model\nEMBED_DIM = 32\nLSTM_OUT = 64\n\nmodel = Sequential()\nmodel.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\nmodel.add(LSTM(LSTM_OUT))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nprint(model.summary())\n","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:50:15.021795Z","iopub.execute_input":"2022-01-02T17:50:15.022126Z","iopub.status.idle":"2022-01-02T17:50:15.298563Z","shell.execute_reply.started":"2022-01-02T17:50:15.022091Z","shell.execute_reply":"2022-01-02T17:50:15.297338Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'models/LSTM.h5',\n    monitor='accuracy',\n    save_best_only=True,\n    verbose=1\n)\n\n# Final evaluation of the model\n\nmodel.fit(x_train, y_train, batch_size = 256, epochs = 5, callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:50:46.812756Z","iopub.execute_input":"2022-01-02T17:50:46.813088Z","iopub.status.idle":"2022-01-02T17:53:45.738773Z","shell.execute_reply.started":"2022-01-02T17:50:46.813053Z","shell.execute_reply":"2022-01-02T17:53:45.738070Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"Once the model is created, then we can test the performance on unseen reviews. ","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(x_test, batch_size = 128)\n\ntrue = 0\nfor i, y in enumerate(y_test):\n    if y == y_pred[i]:\n        true += 1\n\nprint('Correct Prediction: {}'.format(true))\nprint('Wrong Prediction: {}'.format(len(y_pred) - true))\nprint('Accuracy: {}'.format(true/len(y_pred)*100))","metadata":{"execution":{"iopub.status.busy":"2022-01-02T17:57:41.126272Z","iopub.execute_input":"2022-01-02T17:57:41.126858Z","iopub.status.idle":"2022-01-02T17:57:48.152537Z","shell.execute_reply.started":"2022-01-02T17:57:41.126816Z","shell.execute_reply":"2022-01-02T17:57:48.151391Z"},"trusted":true},"execution_count":117,"outputs":[]}]}